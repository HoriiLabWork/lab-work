{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lLN5JTga96SM"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import os\n",
        "import ssl\n",
        "import re\n",
        "import random\n",
        "from urllib import request\n",
        "import cv2\n",
        "import numpy as np\n",
        "import ffmpeg\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rkn/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qNELYQzrBFcl"
      },
      "outputs": [],
      "source": [
        "UCF_ROOT_URL = 'https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/'\n",
        "context = ssl._create_unverified_context()\n",
        "\n",
        "def ucf_label2name_dict():\n",
        "  idx = request.urlopen(UCF_ROOT_URL, context=context).read().decode('utf-8')\n",
        "  videos = sorted(list(set(re.findall('(v_[\\w_]+\\.avi)', idx))))\n",
        "  output = {}\n",
        "  for video in videos:\n",
        "    label = re.findall('v_(.*)_g', video)[0]\n",
        "    output.setdefault(label, []).append(video)\n",
        "  return output\n",
        "\n",
        "def fetch_ucf_video(video, destination):\n",
        "  urlpath = request.urljoin(UCF_ROOT_URL, video)\n",
        "  print(f'Fetching {urlpath} -> {destination}')\n",
        "  data = request.urlopen(urlpath, context=context).read()\n",
        "  open(destination, \"wb\").write(data)\n",
        "  return destination\n",
        "\n",
        "def prepare_ucf_dataset(destination='ucf101', num_per_class=None, random_sample=False):\n",
        "  cnt = 0\n",
        "  for label, videos in tqdm(ucf_label2name_dict().items()):\n",
        "    if num_per_class:\n",
        "      if random_sample:\n",
        "        videos = random.sample(videos, num_per_class)\n",
        "      else:\n",
        "        videos = videos[:num_per_class]\n",
        "    for video in videos:\n",
        "      data_name = re.findall('(.*).avi', video)[0]\n",
        "      data_dir = os.path.join(destination, label, data_name)\n",
        "      os.makedirs(data_dir, exist_ok=True)\n",
        "      video_dest = os.path.join(data_dir, video)\n",
        "      audio_dest = os.path.join(data_dir, video.replace('.avi', '.wav'))\n",
        "      if not os.path.exists(video_dest):\n",
        "        fetch_ucf_video(video, video_dest)\n",
        "        try:\n",
        "          ffmpeg.run(ffmpeg.output(ffmpeg.input(video_dest), audio_dest))\n",
        "        except:\n",
        "          shutil.rmtree(os.path.join(destination, label))\n",
        "          break\n",
        "        cnt += 1\n",
        "  return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzxIPJDdMTOo",
        "outputId": "c71cc17d-b31e-4242-9697-41677d3f79f2"
      },
      "outputs": [],
      "source": [
        "# print('video count:', prepare_ucf_dataset(num_per_class=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DRDJAg5kUK33"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from pytorchvideo.transforms import Permute\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def crop_center_square(frame):\n",
        "  h, w = frame.shape[0:2]\n",
        "  min_dim = min(h, w)\n",
        "  sx = (w//2) - (min_dim//2)\n",
        "  sy = (h//2) - (min_dim//2)\n",
        "  return frame[sy : sy+min_dim, sx : sx+min_dim]\n",
        "\n",
        "def load_video(path, n_frames=None):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames=[]\n",
        "  frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "  if not n_frames:\n",
        "    n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "  try:\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret: break\n",
        "      frame = crop_center_square(frame)\n",
        "      frame = cv2.resize(frame, (224, 224))\n",
        "      frame = frame[:, :, [2, 1, 0]]\n",
        "      frames.append(frame)\n",
        "      if n_frames:\n",
        "        if len(frames) == n_frames : break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  video_tensor = torch.tensor(frames) / 255.0\n",
        "  video_tensor = torch.permute(video_tensor, (0, 3, 1, 2)) # THWC -> TCHW\n",
        "  video_shape = video_tensor.shape\n",
        "  if video_shape[0] < n_frames:\n",
        "    pad = torch.zeros((n_frames - video_shape[0], video_shape[1], video_shape[2], video_shape[3]))\n",
        "    video_tensor = torch.concat((video_tensor, pad))\n",
        "  return video_tensor, frame_rate\n",
        "\n",
        "def load_audio(path, samples_per_frame=None, video_frame_rate=None, n_video_frames=None):\n",
        "  waveform, raw_sample_rate = torchaudio.load(path)\n",
        "  if samples_per_frame and video_frame_rate:\n",
        "    sample_rate = samples_per_frame * video_frame_rate\n",
        "    waveform = torchaudio.transforms.Resample(raw_sample_rate, sample_rate)(waveform)\n",
        "  else: sample_rate = raw_sample_rate\n",
        "  if waveform.shape[0] != 1:\n",
        "    waveform = torch.mean(waveform, dim=0) #stereo -> mono\n",
        "  waveform = waveform.squeeze().unsqueeze(1)\n",
        "  if samples_per_frame and n_video_frames:\n",
        "    n_samples = samples_per_frame * n_video_frames\n",
        "    if waveform.shape[0] < n_samples:\n",
        "      pad = torch.zeros(n_samples - waveform.shape[0], 1)\n",
        "      waveform = torch.concat((waveform, pad))\n",
        "    else:\n",
        "      waveform = waveform[:n_samples]\n",
        "  return waveform, sample_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "25.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_860326/3423768512.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  video_tensor = torch.tensor(frames) / 255.0\n"
          ]
        }
      ],
      "source": [
        "video, fps = load_video('ucf101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01/v_ApplyEyeMakeup_g01_c01.avi', 16)\n",
        "print(video.shape)\n",
        "print(fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30720, 1])\n",
            "48000.0\n"
          ]
        }
      ],
      "source": [
        "audio, sample_rate = load_audio('ucf101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01/v_ApplyEyeMakeup_g01_c01.wav', samples_per_frame=1920, video_frame_rate=fps, n_video_frames=16)\n",
        "print(audio.shape)\n",
        "print(sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def loader(path, n_frames, audio_samples_per_frame):\n",
        "  datum = os.listdir(path)\n",
        "  for data in datum:\n",
        "    if data.endswith('.avi'):\n",
        "      video, fps = load_video(os.path.join(path, data), n_frames=n_frames)\n",
        "  for data in datum:\n",
        "    if data.endswith('.wav'):\n",
        "      audio, _ = load_audio(os.path.join(path, data),\n",
        "                            samples_per_frame=audio_samples_per_frame,\n",
        "                            video_frame_rate=fps,\n",
        "                            n_video_frames=n_frames)\n",
        "  return video, audio\n",
        "\n",
        "\n",
        "class UCFDataset(DatasetFolder):\n",
        "  def __init__(self, root, loader, n_video_frames=None, audio_samples_per_frame=None):\n",
        "    self.root = root\n",
        "    self.loader = loader\n",
        "    self.n_frames = n_video_frames\n",
        "    self.audio_samples_per_frames = audio_samples_per_frame\n",
        "    self.classes, self.class_to_idx = super().find_classes(root)\n",
        "    self.samples = self.make_dataset(self.root, self.class_to_idx)\n",
        "\n",
        "  def make_dataset(self, directory, class_to_idx):\n",
        "    instances = []\n",
        "    for target_class in sorted(class_to_idx.keys()):\n",
        "      class_idx = class_to_idx[target_class]\n",
        "      target_dir = os.path.join(directory, target_class)\n",
        "      if not os.path.isdir(target_dir):\n",
        "        continue\n",
        "      for data_name in sorted(os.listdir(target_dir)):\n",
        "        path = os.path.join(target_dir, data_name)\n",
        "        # item = path, F.one_hot(torch.tensor(class_idx), num_classes=self.num_labels)\n",
        "        item = path, torch.tensor(class_idx)\n",
        "        instances.append(item)\n",
        "    return instances\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path, target = self.samples[idx]\n",
        "    (video, audio) = self.loader(path, self.n_frames, self.audio_samples_per_frames)\n",
        "    return video, audio, target\n",
        "  \n",
        "  @property\n",
        "  def num_labels(self):\n",
        "    return len(os.listdir(self.root))\n",
        "  \n",
        "  @property\n",
        "  def id2label(self):\n",
        "    return {v:k for k,v in self.class_to_idx.items()}\n",
        "  \n",
        "  @property\n",
        "  def label2id(self):\n",
        "    return self.class_to_idx\n",
        "\n",
        "\n",
        "class SetTransform(Dataset):\n",
        "  def __init__(self, dataset, transform):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    video, audio, label = self.dataset[idx]\n",
        "    video = self.transform(video)\n",
        "    return video, audio, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6586"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.transforms import (Compose,\n",
        "                                    Resize)\n",
        "from transformers import PerceiverForMultimodalAutoencoding\n",
        "\n",
        "pretrained_model = PerceiverForMultimodalAutoencoding.from_pretrained(\"deepmind/multimodal-perceiver\")\n",
        "all_dataset = UCFDataset('ucf101',\n",
        "                         loader=loader,\n",
        "                         n_video_frames=pretrained_model.config.num_frames,\n",
        "                         audio_samples_per_frame=pretrained_model.config.audio_samples_per_frame)\n",
        "len(all_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 4586\n",
            "validation: 1000\n",
            "test: 1000\n"
          ]
        }
      ],
      "source": [
        "val_size = 1000\n",
        "test_size = 1000\n",
        "train_size = len(all_dataset) - val_size - test_size\n",
        "\n",
        "seed = 0\n",
        "test_dataset, trainval_dataset = torch.utils.data.random_split(all_dataset, [test_size, train_size + val_size], generator=torch.Generator().manual_seed(seed))\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "print('train:', len(train_dataset))\n",
        "print('validation:', len(val_dataset))\n",
        "print('test:', len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "torch.Size([30720, 1])\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "video, audio, label = all_dataset[0]\n",
        "print(video.shape)\n",
        "print(audio.shape)\n",
        "print(label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    videos, audios, labels = zip(*examples)\n",
        "    videos = torch.stack(videos)\n",
        "    audios = torch.stack(audios)\n",
        "    onehot_labels = F.one_hot(torch.stack(labels), num_classes=all_dataset.num_labels)\n",
        "    labels = torch.stack(labels)\n",
        "    subsampled_output_points = {\n",
        "        \"image\": torch.arange(0, 0),\n",
        "        \"audio\": torch.arange(0, 0),\n",
        "        \"label\": None,\n",
        "    }\n",
        "    return dict(image=videos,\n",
        "                audio=audios,\n",
        "                # label=onehot_labels,\n",
        "                labels=labels,\n",
        "                subsampled_output_points=subsampled_output_points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " image\n",
            "\t torch.Size([4, 16, 3, 224, 224])\n",
            " audio\n",
            "\t torch.Size([4, 30720, 1])\n",
            " labels\n",
            "\t torch.Size([4])\n",
            " subsampled_output_points\n",
            "\t image\n",
            "\t\t torch.Size([0])\n",
            "\t audio\n",
            "\t\t torch.Size([0])\n",
            "\t label\n",
            "\t\t None\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4)\n",
        "\n",
        "batch = next(iter(train_dataloader))\n",
        "def print_dict(d, c):\n",
        "    for k,v in d.items():\n",
        "        if isinstance(v, dict):\n",
        "            print('\\t'*c, k)\n",
        "            print_dict(v, c+1)\n",
        "        elif isinstance(v, torch.Tensor):\n",
        "            print('\\t'*c, k)\n",
        "            print('\\t'*(c+1), v.shape)\n",
        "        else:\n",
        "            print('\\t'*c, k)\n",
        "            print('\\t'*(c+1), v)\n",
        "print_dict(batch, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers.models.perceiver.modeling_perceiver import PerceiverClassifierOutput\n",
        "\n",
        "class TrainablePerceiverForMultimodalAutoencoding(PerceiverForMultimodalAutoencoding):\n",
        "    def forward(\n",
        "        self,\n",
        "        image=None,\n",
        "        audio=None,\n",
        "        attention_mask=None,\n",
        "        subsampled_output_points=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        return_dict = self.config.use_return_dict\n",
        "        inputs = {\n",
        "            'image': image,\n",
        "            'audio': audio,\n",
        "            'label': F.one_hot(labels, num_classes=self.config.num_labels),\n",
        "        }\n",
        "        outputs = self.perceiver(\n",
        "            inputs=inputs,\n",
        "            attention_mask=attention_mask,\n",
        "            subsampled_output_points=subsampled_output_points,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        logits = outputs.logits if return_dict else outputs[0]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits['label'].view(-1, self.config.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return PerceiverClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of TrainablePerceiverForMultimodalAutoencoding were not initialized from the model checkpoint at deepmind/multimodal-perceiver and are newly initialized because the shapes did not match:\n",
            "- perceiver.input_preprocessor.padding.audio: found shape torch.Size([1, 303]) in the checkpoint and torch.Size([1, 4]) in the model instantiated\n",
            "- perceiver.input_preprocessor.padding.image: found shape torch.Size([1, 461]) in the checkpoint and torch.Size([1, 162]) in the model instantiated\n",
            "- perceiver.input_preprocessor.padding.label: found shape torch.Size([1, 4]) in the checkpoint and torch.Size([1, 356]) in the model instantiated\n",
            "- perceiver.input_preprocessor.mask.audio: found shape torch.Size([1, 704]) in the checkpoint and torch.Size([1, 405]) in the model instantiated\n",
            "- perceiver.input_preprocessor.mask.image: found shape torch.Size([1, 704]) in the checkpoint and torch.Size([1, 405]) in the model instantiated\n",
            "- perceiver.input_preprocessor.mask.label: found shape torch.Size([1, 704]) in the checkpoint and torch.Size([1, 405]) in the model instantiated\n",
            "- perceiver.output_postprocessor.modalities.label.classifier.weight: found shape torch.Size([700, 512]) in the checkpoint and torch.Size([49, 512]) in the model instantiated\n",
            "- perceiver.output_postprocessor.modalities.label.classifier.bias: found shape torch.Size([700]) in the checkpoint and torch.Size([49]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.layernorm2.weight: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.layernorm2.bias: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.query.weight: found shape torch.Size([704, 512]) in the checkpoint and torch.Size([405, 512]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.query.bias: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.key.weight: found shape torch.Size([704, 704]) in the checkpoint and torch.Size([405, 405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.key.bias: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.value.weight: found shape torch.Size([704, 704]) in the checkpoint and torch.Size([405, 405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.value.bias: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.output.dense.weight: found shape torch.Size([512, 704]) in the checkpoint and torch.Size([512, 405]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = TrainablePerceiverForMultimodalAutoencoding.from_pretrained(\"deepmind/multimodal-perceiver\",\n",
        "                                                                    num_labels=all_dataset.num_labels,\n",
        "                                                                    id2label=all_dataset.id2label,\n",
        "                                                                    label2id=all_dataset.label2id,\n",
        "                                                                    ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "metric_name = \"accuracy\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"perceiver-ucf\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    logging_dir='logs',\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_860326/1052707893.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# inputs = next(iter(train_dataloader))\n",
        "# nchunks = 128\n",
        "# image_chunk_size = np.prod((16, 224, 224)) // nchunks\n",
        "# audio_chunk_size = audio.shape[1] // model.config.samples_per_patch // nchunks\n",
        "# print(model.config.samples_per_patch)\n",
        "# # process the first chunk\n",
        "# chunk_idx = 0\n",
        "# subsampling = {\n",
        "#     \"image\": torch.arange(0, 0),\n",
        "#     \"audio\": torch.arange(0, 0),\n",
        "#     \"label\": None,\n",
        "# }\n",
        "# print(inputs['inputs']['audio'].shape)\n",
        "\n",
        "# outputs = model(**inputs, subsampled_output_points=subsampling)\n",
        "# print(outputs.logits['audio'].shape)\n",
        "# print(outputs.logits['image'].shape)\n",
        "# print(outputs.logits['label'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rkn/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 4586\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3441\n",
            "  Number of trainable parameters = 18552896\n",
            "  0%|          | 0/3441 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            " 15%|█▍        | 500/3441 [19:15<1:52:06,  2.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9216, 'learning_rate': 4.2734670154025e-05, 'epoch': 0.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 1000/3441 [38:34<1:32:08,  2.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9057, 'learning_rate': 3.5469340308049985e-05, 'epoch': 0.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1147/3441 [44:10<1:13:35,  1.92s/it]***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 4\n"
          ]
        },
        {
          "ename": "AxisError",
          "evalue": "axis 1 is out of bounds for array of dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:1883\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1880\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1883\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1885\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   1886\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1887\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:2131\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2125\u001b[0m             metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[1;32m   2126\u001b[0m                 eval_dataset\u001b[39m=\u001b[39meval_dataset,\n\u001b[1;32m   2127\u001b[0m                 ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2128\u001b[0m                 metric_key_prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00meval_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2129\u001b[0m             )\n\u001b[1;32m   2130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2131\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:2827\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2824\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2826\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2827\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2828\u001b[0m     eval_dataloader,\n\u001b[1;32m   2829\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2830\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2831\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2832\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2833\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2834\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2835\u001b[0m )\n\u001b[1;32m   2837\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:3116\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3112\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3113\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   3114\u001b[0m         )\n\u001b[1;32m   3115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3116\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   3117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3118\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
            "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(eval_pred):\n\u001b[1;32m      7\u001b[0m     predictions, labels \u001b[39m=\u001b[39m eval_pred\n\u001b[0;32m----> 8\u001b[0m     predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(predictions, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mlabels)\n",
            "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1216\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "lab-work-gFKfzxI5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "849f61b40a9f0695613f10abe0801a9209e9c807f7f85006c3621f09dc21595f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
