{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lLN5JTga96SM"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import os\n",
        "import ssl\n",
        "import re\n",
        "import random\n",
        "from urllib import request\n",
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "import scipy.io.wavfile\n",
        "import ffmpeg\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qNELYQzrBFcl"
      },
      "outputs": [],
      "source": [
        "UCF_ROOT_URL = 'https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/'\n",
        "context = ssl._create_unverified_context()\n",
        "\n",
        "def ucf_label2name_dict():\n",
        "  idx = request.urlopen(UCF_ROOT_URL, context=context).read().decode('utf-8')\n",
        "  videos = sorted(list(set(re.findall('(v_[\\w_]+\\.avi)', idx))))\n",
        "  output = {}\n",
        "  for video in videos:\n",
        "    label = re.findall('v_(.*)_g', video)[0]\n",
        "    output.setdefault(label, []).append(video)\n",
        "  return output\n",
        "\n",
        "def fetch_ucf_video(video, destination):\n",
        "  urlpath = request.urljoin(UCF_ROOT_URL, video)\n",
        "  print(f'Fetching {urlpath} -> {destination}')\n",
        "  data = request.urlopen(urlpath, context=context).read()\n",
        "  open(destination, \"wb\").write(data)\n",
        "  return destination\n",
        "\n",
        "def prepare_ucf_dataset(destination='ucf101', num_per_class=None, random_sample=False):\n",
        "  cnt = 0\n",
        "  for label, videos in tqdm(ucf_label2name_dict().items()):\n",
        "    if num_per_class:\n",
        "      if random_sample:\n",
        "        videos = random.sample(videos, num_per_class)\n",
        "      else:\n",
        "        videos = videos[:num_per_class]\n",
        "    for video in videos:\n",
        "      data_name = re.findall('(.*).avi', video)[0]\n",
        "      data_dir = os.path.join(destination, label, data_name)\n",
        "      os.makedirs(data_dir, exist_ok=True)\n",
        "      video_dest = os.path.join(data_dir, video)\n",
        "      audio_dest = os.path.join(data_dir, video.replace('.avi', '.wav'))\n",
        "      if not os.path.exists(video_dest):\n",
        "        fetch_ucf_video(video, video_dest)\n",
        "        try:\n",
        "          ffmpeg.run(ffmpeg.output(ffmpeg.input(video_dest), audio_dest))\n",
        "        except:\n",
        "          shutil.rmtree(os.path.join(destination, label))\n",
        "          break\n",
        "        cnt += 1\n",
        "  return cnt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.5h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzxIPJDdMTOo",
        "outputId": "c71cc17d-b31e-4242-9697-41677d3f79f2"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mvideo count:\u001b[39m\u001b[39m'\u001b[39m, prepare_ucf_dataset(num_per_class\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m))\n",
            "Cell \u001b[0;32mIn[2], line 22\u001b[0m, in \u001b[0;36mprepare_ucf_dataset\u001b[0;34m(destination, num_per_class, random_sample)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_ucf_dataset\u001b[39m(destination\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mucf101\u001b[39m\u001b[39m'\u001b[39m, num_per_class\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, random_sample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m   cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 22\u001b[0m   \u001b[39mfor\u001b[39;00m label, videos \u001b[39min\u001b[39;00m tqdm(ucf_label2name_dict()\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m num_per_class:\n\u001b[1;32m     24\u001b[0m       \u001b[39mif\u001b[39;00m random_sample:\n",
            "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mucf_label2name_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mucf_label2name_dict\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m   idx \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39;49murlopen(UCF_ROOT_URL, context\u001b[39m=\u001b[39;49mcontext)\u001b[39m.\u001b[39;49mread()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m   videos \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(re\u001b[39m.\u001b[39mfindall(\u001b[39m'\u001b[39m\u001b[39m(v_[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw_]+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.avi)\u001b[39m\u001b[39m'\u001b[39m, idx))))\n\u001b[1;32m      7\u001b[0m   output \u001b[39m=\u001b[39m {}\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked:\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_chunked(amt)\n\u001b[1;32m    461\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/http/client.py:591\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m=\u001b[39m chunk_left \u001b[39m-\u001b[39m amt\n\u001b[1;32m    589\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m value\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(chunk_left))\n\u001b[1;32m    592\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     amt \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m chunk_left\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/http/client.py:630\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[1;32m    624\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[1;32m    626\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[1;32m    632\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print('video count:', prepare_ucf_dataset(num_per_class=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ApplyEyeMakeup 145\n",
            "ApplyLipstick 114\n",
            "Archery 145\n",
            "BabyCrawling 132\n",
            "BalanceBeam 108\n",
            "BandMarching 155\n",
            "BlowDryHair 131\n",
            "BlowingCandles 109\n",
            "BodyWeightSquats 112\n",
            "Bowling 155\n",
            "BoxingPunchingBag 163\n",
            "BoxingSpeedBag 134\n",
            "BrushingTeeth 131\n",
            "CliffDiving 138\n",
            "CricketBowling 139\n",
            "CricketShot 167\n",
            "CuttingInKitchen 110\n",
            "FieldHockeyPenalty 126\n",
            "FloorGymnastics 125\n",
            "FrisbeeCatch 126\n",
            "FrontCrawl 137\n",
            "Haircut 130\n",
            "HammerThrow 150\n",
            "Hammering 140\n",
            "HandstandWalking 111\n",
            "HeadMassage 147\n",
            "IceDancing 158\n",
            "Knitting 123\n",
            "LongJump 131\n",
            "MoppingFloor 110\n",
            "ParallelBars 114\n",
            "PlayingCello 164\n",
            "PlayingDaf 151\n",
            "PlayingDhol 164\n",
            "PlayingFlute 155\n",
            "PlayingSitar 157\n",
            "Rafting 111\n",
            "ShavingBeard 161\n",
            "Shotput 144\n",
            "SkyDiving 110\n",
            "SoccerPenalty 137\n",
            "StillRings 112\n",
            "SumoWrestling 116\n",
            "Surfing 126\n",
            "TableTennisShot 140\n",
            "Typing 136\n",
            "UnevenBars 104\n",
            "WallPushups 130\n",
            "WritingOnBoard 152\n",
            "49\n"
          ]
        }
      ],
      "source": [
        "cnt=0\n",
        "for label in sorted(os.listdir('ucf101')):\n",
        "    print(label, len(os.listdir(os.path.join('ucf101', label))))\n",
        "    cnt+=1\n",
        "print(cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DRDJAg5kUK33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rkn/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from pytorchvideo.transforms import Permute\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def crop_center_square(frame):\n",
        "  h, w = frame.shape[0:2]\n",
        "  min_dim = min(h, w)\n",
        "  sx = (w//2) - (min_dim//2)\n",
        "  sy = (h//2) - (min_dim//2)\n",
        "  return frame[sy : sy+min_dim, sx : sx+min_dim]\n",
        "\n",
        "def load_video(path, n_frames=None):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames=[]\n",
        "  frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "  if not n_frames:\n",
        "    n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "  try:\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret: break\n",
        "      frame = crop_center_square(frame)\n",
        "      frame = cv2.resize(frame, (224, 224))\n",
        "      frame = frame[:, :, [2, 1, 0]]\n",
        "      frames.append(frame)\n",
        "      if n_frames:\n",
        "        if len(frames) == n_frames : break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  video_tensor = torch.tensor(frames) / 255.0\n",
        "  video_tensor = torch.permute(video_tensor, (0, 3, 1, 2))\n",
        "  video_shape = video_tensor.shape\n",
        "  if video_shape[0] < n_frames:\n",
        "    pad = torch.zeros((n_frames - video_shape[0], video_shape[1], video_shape[2], video_shape[3]))\n",
        "    video_tensor = torch.concat((video_tensor, pad))\n",
        "  return video_tensor, n_frames / frame_rate\n",
        "\n",
        "def load_audio(path, len_time=None, sample_rate=None):\n",
        "  waveform, raw_sample_rate = torchaudio.load(path)\n",
        "  if sample_rate:\n",
        "    waveform = torchaudio.transforms.Resample(raw_sample_rate, sample_rate)(waveform)\n",
        "  else: sample_rate = raw_sample_rate\n",
        "  if waveform.shape[0] != 1:\n",
        "    waveform = torch.mean(waveform, dim=0) #stereo -> mono\n",
        "  waveform = waveform.squeeze()\n",
        "  if len_time:\n",
        "    n_frames = int(len_time * sample_rate)\n",
        "    waveform = waveform[:n_frames]\n",
        "  if waveform.shape[0] < n_frames:\n",
        "    pad = torch.zeros(n_frames - waveform.shape[0])\n",
        "    waveform = torch.concat((waveform, pad))\n",
        "  return waveform.unsqueeze(0).unsqueeze(-1), sample_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l=[]\n",
        "fps = []\n",
        "dir = list(os.walk('ucf101'))\n",
        "for root, dirs, files in tqdm(dir):\n",
        "    for f in files:\n",
        "        video = os.path.join(root, f)\n",
        "        if video.endswith('.avi'):\n",
        "            cap = cv2.VideoCapture(video)\n",
        "            l.append(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            fps.append(cap.get(cv2.CAP_PROP_FPS))\n",
        "            cap.release()\n",
        "print(sorted(l))\n",
        "print(sorted(fps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[[0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           [0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           [0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           ...,\n",
              "           [0.0549, 0.0784, 0.0784,  ..., 0.4627, 0.4745, 0.4784],\n",
              "           [0.0157, 0.0314, 0.1569,  ..., 0.4627, 0.4745, 0.4784],\n",
              "           [0.0745, 0.0745, 0.2392,  ..., 0.4627, 0.4745, 0.4784]],\n",
              " \n",
              "          [[0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           [0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           [0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           ...,\n",
              "           [0.0549, 0.0784, 0.0784,  ..., 0.4627, 0.4745, 0.4784],\n",
              "           [0.0157, 0.0314, 0.1569,  ..., 0.4627, 0.4745, 0.4784],\n",
              "           [0.0745, 0.0745, 0.2392,  ..., 0.4627, 0.4745, 0.4784]],\n",
              " \n",
              "          [[0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           [0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           [0.2471, 0.2471, 0.2431,  ..., 0.9294, 0.9294, 0.9294],\n",
              "           ...,\n",
              "           [0.0549, 0.0784, 0.0784,  ..., 0.4275, 0.4353, 0.4471],\n",
              "           [0.0157, 0.0314, 0.1569,  ..., 0.4275, 0.4353, 0.4471],\n",
              "           [0.0745, 0.0745, 0.2392,  ..., 0.4275, 0.4353, 0.4471]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0.3686, 0.3686, 0.3686,  ..., 0.9451, 0.9490, 0.9490],\n",
              "           [0.3725, 0.3686, 0.3765,  ..., 0.9373, 0.9451, 0.9451],\n",
              "           [0.3765, 0.3725, 0.3765,  ..., 0.9255, 0.9294, 0.9333],\n",
              "           ...,\n",
              "           [0.1529, 0.1569, 0.1765,  ..., 0.6431, 0.6471, 0.6471],\n",
              "           [0.0588, 0.0863, 0.2275,  ..., 0.6431, 0.6471, 0.6471],\n",
              "           [0.1020, 0.1059, 0.2745,  ..., 0.6431, 0.6471, 0.6471]],\n",
              " \n",
              "          [[0.3765, 0.3765, 0.3725,  ..., 0.9490, 0.9490, 0.9490],\n",
              "           [0.3765, 0.3765, 0.3725,  ..., 0.9451, 0.9490, 0.9490],\n",
              "           [0.3765, 0.3765, 0.3725,  ..., 0.9333, 0.9373, 0.9373],\n",
              "           ...,\n",
              "           [0.1529, 0.1569, 0.1765,  ..., 0.6471, 0.6471, 0.6471],\n",
              "           [0.0588, 0.0863, 0.2275,  ..., 0.6471, 0.6471, 0.6471],\n",
              "           [0.1020, 0.1059, 0.2745,  ..., 0.6471, 0.6471, 0.6471]],\n",
              " \n",
              "          [[0.3686, 0.3686, 0.3686,  ..., 0.9373, 0.9490, 0.9490],\n",
              "           [0.3686, 0.3686, 0.3686,  ..., 0.9333, 0.9490, 0.9490],\n",
              "           [0.3686, 0.3686, 0.3686,  ..., 0.9216, 0.9333, 0.9373],\n",
              "           ...,\n",
              "           [0.1529, 0.1569, 0.1765,  ..., 0.6431, 0.6471, 0.6471],\n",
              "           [0.0588, 0.0863, 0.2275,  ..., 0.6431, 0.6471, 0.6471],\n",
              "           [0.1020, 0.1059, 0.2745,  ..., 0.6431, 0.6471, 0.6471]]],\n",
              " \n",
              " \n",
              "         [[[0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           [0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           [0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           ...,\n",
              "           [0.0314, 0.0549, 0.0667,  ..., 0.3765, 0.3882, 0.3922],\n",
              "           [0.0000, 0.0078, 0.1451,  ..., 0.3765, 0.3882, 0.3922],\n",
              "           [0.0510, 0.0510, 0.2275,  ..., 0.3765, 0.3882, 0.3922]],\n",
              " \n",
              "          [[0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           [0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           [0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           ...,\n",
              "           [0.0314, 0.0549, 0.0667,  ..., 0.3765, 0.3882, 0.3922],\n",
              "           [0.0000, 0.0078, 0.1451,  ..., 0.3765, 0.3882, 0.3922],\n",
              "           [0.0510, 0.0510, 0.2275,  ..., 0.3765, 0.3882, 0.3922]],\n",
              " \n",
              "          [[0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           [0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           [0.2549, 0.2549, 0.2510,  ..., 0.7608, 0.7608, 0.7608],\n",
              "           ...,\n",
              "           [0.0314, 0.0549, 0.0667,  ..., 0.3412, 0.3490, 0.3608],\n",
              "           [0.0000, 0.0078, 0.1451,  ..., 0.3412, 0.3490, 0.3608],\n",
              "           [0.0510, 0.0510, 0.2275,  ..., 0.3412, 0.3490, 0.3608]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0.3412, 0.3412, 0.3294,  ..., 0.8431, 0.8353, 0.8353],\n",
              "           [0.3451, 0.3412, 0.3373,  ..., 0.8353, 0.8314, 0.8314],\n",
              "           [0.3451, 0.3412, 0.3333,  ..., 0.8314, 0.8275, 0.8314],\n",
              "           ...,\n",
              "           [0.1294, 0.1333, 0.1647,  ..., 0.5255, 0.5294, 0.5294],\n",
              "           [0.0353, 0.0627, 0.2157,  ..., 0.5255, 0.5294, 0.5294],\n",
              "           [0.0784, 0.0824, 0.2627,  ..., 0.5255, 0.5294, 0.5294]],\n",
              " \n",
              "          [[0.3373, 0.3373, 0.3255,  ..., 0.8353, 0.8353, 0.8353],\n",
              "           [0.3373, 0.3373, 0.3255,  ..., 0.8314, 0.8353, 0.8353],\n",
              "           [0.3333, 0.3333, 0.3216,  ..., 0.8314, 0.8353, 0.8353],\n",
              "           ...,\n",
              "           [0.1294, 0.1333, 0.1647,  ..., 0.5294, 0.5294, 0.5294],\n",
              "           [0.0353, 0.0627, 0.2157,  ..., 0.5294, 0.5294, 0.5294],\n",
              "           [0.0784, 0.0824, 0.2627,  ..., 0.5294, 0.5294, 0.5294]],\n",
              " \n",
              "          [[0.3412, 0.3412, 0.3294,  ..., 0.8353, 0.8353, 0.8353],\n",
              "           [0.3412, 0.3412, 0.3294,  ..., 0.8314, 0.8353, 0.8353],\n",
              "           [0.3412, 0.3412, 0.3294,  ..., 0.8275, 0.8314, 0.8353],\n",
              "           ...,\n",
              "           [0.1294, 0.1333, 0.1647,  ..., 0.5255, 0.5294, 0.5294],\n",
              "           [0.0353, 0.0627, 0.2157,  ..., 0.5255, 0.5294, 0.5294],\n",
              "           [0.0784, 0.0824, 0.2627,  ..., 0.5255, 0.5294, 0.5294]]],\n",
              " \n",
              " \n",
              "         [[[0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           [0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           [0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           ...,\n",
              "           [0.0588, 0.0824, 0.0902,  ..., 0.3882, 0.4000, 0.4039],\n",
              "           [0.0196, 0.0353, 0.1686,  ..., 0.3882, 0.4000, 0.4039],\n",
              "           [0.0784, 0.0784, 0.2510,  ..., 0.3882, 0.4000, 0.4039]],\n",
              " \n",
              "          [[0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           [0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           [0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           ...,\n",
              "           [0.0588, 0.0824, 0.0902,  ..., 0.3882, 0.4000, 0.4039],\n",
              "           [0.0196, 0.0353, 0.1686,  ..., 0.3882, 0.4000, 0.4039],\n",
              "           [0.0784, 0.0784, 0.2510,  ..., 0.3882, 0.4000, 0.4039]],\n",
              " \n",
              "          [[0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           [0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           [0.3176, 0.3176, 0.3137,  ..., 0.8078, 0.8000, 0.8000],\n",
              "           ...,\n",
              "           [0.0588, 0.0824, 0.0902,  ..., 0.3608, 0.3686, 0.3804],\n",
              "           [0.0196, 0.0353, 0.1686,  ..., 0.3608, 0.3686, 0.3804],\n",
              "           [0.0784, 0.0784, 0.2510,  ..., 0.3608, 0.3686, 0.3804]],\n",
              " \n",
              "          ...,\n",
              " \n",
              "          [[0.4588, 0.4588, 0.4510,  ..., 0.8667, 0.8627, 0.8627],\n",
              "           [0.4627, 0.4588, 0.4588,  ..., 0.8588, 0.8588, 0.8588],\n",
              "           [0.4745, 0.4706, 0.4667,  ..., 0.8549, 0.8510, 0.8549],\n",
              "           ...,\n",
              "           [0.1569, 0.1608, 0.1882,  ..., 0.5373, 0.5412, 0.5412],\n",
              "           [0.0627, 0.0902, 0.2392,  ..., 0.5373, 0.5412, 0.5412],\n",
              "           [0.1059, 0.1098, 0.2863,  ..., 0.5373, 0.5412, 0.5412]],\n",
              " \n",
              "          [[0.4588, 0.4588, 0.4510,  ..., 0.8627, 0.8627, 0.8627],\n",
              "           [0.4588, 0.4588, 0.4510,  ..., 0.8588, 0.8627, 0.8627],\n",
              "           [0.4667, 0.4667, 0.4588,  ..., 0.8549, 0.8588, 0.8588],\n",
              "           ...,\n",
              "           [0.1569, 0.1608, 0.1882,  ..., 0.5412, 0.5412, 0.5412],\n",
              "           [0.0627, 0.0902, 0.2392,  ..., 0.5412, 0.5412, 0.5412],\n",
              "           [0.1059, 0.1098, 0.2863,  ..., 0.5412, 0.5412, 0.5412]],\n",
              " \n",
              "          [[0.4588, 0.4588, 0.4510,  ..., 0.8588, 0.8627, 0.8627],\n",
              "           [0.4588, 0.4588, 0.4510,  ..., 0.8549, 0.8627, 0.8627],\n",
              "           [0.4588, 0.4588, 0.4510,  ..., 0.8510, 0.8549, 0.8588],\n",
              "           ...,\n",
              "           [0.1569, 0.1608, 0.1882,  ..., 0.5373, 0.5412, 0.5412],\n",
              "           [0.0627, 0.0902, 0.2392,  ..., 0.5373, 0.5412, 0.5412],\n",
              "           [0.1059, 0.1098, 0.2863,  ..., 0.5373, 0.5412, 0.5412]]]]),\n",
              " 4.0)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_video('ucf101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01/v_ApplyEyeMakeup_g01_c01.avi', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 0.0000,  0.0000,  0.0000,  ..., -0.0625, -0.0687, -0.0761]), 44100)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_audio('ucf101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01/v_ApplyEyeMakeup_g01_c01.wav', max_time=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def loader(path, n_frames, sample_rate):\n",
        "  datum = os.listdir(path)\n",
        "  for data in datum:\n",
        "    if data.endswith('.avi'):\n",
        "      video, time = load_video(os.path.join(path, data), n_frames=n_frames)\n",
        "  for data in datum:\n",
        "    if data.endswith('.wav'):\n",
        "      audio, _ = load_audio(os.path.join(path, data), len_time=time, sample_rate=sample_rate)\n",
        "  return video, audio\n",
        "\n",
        "\n",
        "class UCFDataset(DatasetFolder):\n",
        "  def __init__(self, root, loader, n_video_frames=None, audio_sample_rate=None):\n",
        "    self.root = root\n",
        "    self.loader = loader\n",
        "    self.n_frames = n_video_frames\n",
        "    self.sample_rate = audio_sample_rate\n",
        "    self.classes, self.class_to_idx = super().find_classes(root)\n",
        "    self.samples = self.make_dataset(self.root, self.class_to_idx)\n",
        "\n",
        "  def make_dataset(self, directory, class_to_idx):\n",
        "    instances = []\n",
        "    for target_class in sorted(class_to_idx.keys()):\n",
        "      class_idx = class_to_idx[target_class]\n",
        "      target_dir = os.path.join(directory, target_class)\n",
        "      if not os.path.isdir(target_dir):\n",
        "        continue\n",
        "      for data_name in sorted(os.listdir(target_dir)):\n",
        "        path = os.path.join(target_dir, data_name)\n",
        "        item = path, F.one_hot(torch.tensor(class_idx), num_classes=self.num_labels)\n",
        "        instances.append(item)\n",
        "    return instances\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path, target = self.samples[idx]\n",
        "    (video, audio) = self.loader(path, self.n_frames, self.sample_rate)\n",
        "    return video, audio, target\n",
        "  \n",
        "  @property\n",
        "  def num_labels(self):\n",
        "    return len(os.listdir(self.root))\n",
        "  \n",
        "  @property\n",
        "  def id2label(self):\n",
        "    return {v:k for k,v in self.class_to_idx.items()}\n",
        "  \n",
        "  @property\n",
        "  def label2id(self):\n",
        "    return self.class_to_idx\n",
        "\n",
        "\n",
        "class SetTransform(Dataset):\n",
        "  def __init__(self, dataset, transform):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    video, audio, label = self.dataset[idx]\n",
        "    video = self.transform(video)\n",
        "    return video, audio, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6586"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.transforms import (Compose,\n",
        "                                    Resize)\n",
        "\n",
        "all_dataset = UCFDataset('ucf101', loader=loader, n_video_frames=20, audio_sample_rate=16000)\n",
        "len(all_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 4586\n",
            "validation: 1000\n",
            "test: 1000\n"
          ]
        }
      ],
      "source": [
        "val_size = 1000\n",
        "test_size = 1000\n",
        "train_size = len(all_dataset) - val_size - test_size\n",
        "\n",
        "seed = 0\n",
        "test_dataset, trainval_dataset = torch.utils.data.random_split(all_dataset, [test_size, train_size + val_size], generator=torch.Generator().manual_seed(seed))\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "print('train:', len(train_dataset))\n",
        "print('validation:', len(val_dataset))\n",
        "print('test:', len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([20, 3, 224, 224])\n",
            "torch.Size([1, 12800, 1])\n",
            "torch.Size([49])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_57035/3496717594.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  video_tensor = torch.tensor(frames) / 255.0\n"
          ]
        }
      ],
      "source": [
        "video, audio, label = all_dataset[0]\n",
        "print(video.shape)\n",
        "print(audio.shape)\n",
        "print(label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    videos, audios, labels = zip(*examples)\n",
        "    videos = torch.stack(videos)\n",
        "    audios = torch.stack(audios)\n",
        "    labels = torch.stack(labels)\n",
        "    return {'inputs': {'image': videos, 'audio': audios, 'label': labels}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 20, 3, 224, 224])\n",
            "torch.Size([2, 1, 12800, 1])\n",
            "torch.Size([2, 49])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=2)\n",
        "\n",
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "    for _,v_ in v.items():\n",
        "        print(v_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PerceiverForMultimodalAutoencoding were not initialized from the model checkpoint at deepmind/multimodal-perceiver and are newly initialized because the shapes did not match:\n",
            "- perceiver.input_preprocessor.padding.audio: found shape torch.Size([1, 303]) in the checkpoint and torch.Size([1, 4]) in the model instantiated\n",
            "- perceiver.input_preprocessor.padding.image: found shape torch.Size([1, 461]) in the checkpoint and torch.Size([1, 162]) in the model instantiated\n",
            "- perceiver.input_preprocessor.padding.label: found shape torch.Size([1, 4]) in the checkpoint and torch.Size([1, 356]) in the model instantiated\n",
            "- perceiver.input_preprocessor.mask.audio: found shape torch.Size([1, 704]) in the checkpoint and torch.Size([1, 405]) in the model instantiated\n",
            "- perceiver.input_preprocessor.mask.image: found shape torch.Size([1, 704]) in the checkpoint and torch.Size([1, 405]) in the model instantiated\n",
            "- perceiver.input_preprocessor.mask.label: found shape torch.Size([1, 704]) in the checkpoint and torch.Size([1, 405]) in the model instantiated\n",
            "- perceiver.output_postprocessor.modalities.label.classifier.weight: found shape torch.Size([700, 512]) in the checkpoint and torch.Size([49, 512]) in the model instantiated\n",
            "- perceiver.output_postprocessor.modalities.label.classifier.bias: found shape torch.Size([700]) in the checkpoint and torch.Size([49]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.layernorm2.weight: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.layernorm2.bias: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.query.weight: found shape torch.Size([704, 512]) in the checkpoint and torch.Size([405, 512]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.query.bias: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.key.weight: found shape torch.Size([704, 704]) in the checkpoint and torch.Size([405, 405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.key.bias: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.value.weight: found shape torch.Size([704, 704]) in the checkpoint and torch.Size([405, 405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.self.value.bias: found shape torch.Size([704]) in the checkpoint and torch.Size([405]) in the model instantiated\n",
            "- perceiver.encoder.cross_attention.attention.output.dense.weight: found shape torch.Size([512, 704]) in the checkpoint and torch.Size([512, 405]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import PerceiverForMultimodalAutoencoding\n",
        "\n",
        "model = PerceiverForMultimodalAutoencoding.from_pretrained(\"deepmind/multimodal-perceiver\",\n",
        "                                                           num_labels = all_dataset.num_labels,\n",
        "                                                           id2label=all_dataset.id2label,\n",
        "                                                           label2id=all_dataset.label2id,\n",
        "                                                           ignore_mismatched_sizes=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "metric_name = \"accuracy\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"perceiver-ucf\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    logging_dir='logs',\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_57035/1052707893.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rkn/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 4586\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 10\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1377\n",
            "  Number of trainable parameters = 18552896\n",
            "  0%|          | 0/1377 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.86 GiB (GPU 0; 9.77 GiB total capacity; 6.03 GiB already allocated; 1.86 GiB free; 6.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1794\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1795\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1796\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1797\u001b[0m ):\n\u001b[1;32m   1798\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:2539\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2538\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2539\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2542\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/trainer.py:2571\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2569\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2570\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2571\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2572\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2573\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/models/perceiver/modeling_perceiver.py:1956\u001b[0m, in \u001b[0;36mPerceiverForMultimodalAutoencoding.forward\u001b[0;34m(self, inputs, attention_mask, subsampled_output_points, head_mask, output_attentions, output_hidden_states, labels, return_dict)\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m \u001b[39m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[39m[1, 700]\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1954\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1956\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperceiver(\n\u001b[1;32m   1957\u001b[0m     inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m   1958\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1959\u001b[0m     subsampled_output_points\u001b[39m=\u001b[39;49msubsampled_output_points,\n\u001b[1;32m   1960\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1961\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1962\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1963\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1964\u001b[0m )\n\u001b[1;32m   1965\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits \u001b[39mif\u001b[39;00m return_dict \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1967\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/models/perceiver/modeling_perceiver.py:893\u001b[0m, in \u001b[0;36mPerceiverModel.forward\u001b[0;34m(self, inputs, attention_mask, subsampled_output_points, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    889\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_blocks \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_self_attends_per_block)\n\u001b[1;32m    891\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m--> 893\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    894\u001b[0m     embedding_output,\n\u001b[1;32m    895\u001b[0m     attention_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    896\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    897\u001b[0m     inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    898\u001b[0m     inputs_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    899\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    900\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    901\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    902\u001b[0m )\n\u001b[1;32m    903\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    905\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/models/perceiver/modeling_perceiver.py:559\u001b[0m, in \u001b[0;36mPerceiverEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, inputs, inputs_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    556\u001b[0m all_cross_attentions \u001b[39m=\u001b[39m () \u001b[39mif\u001b[39;00m output_attentions \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39m# Apply the cross-attention between the latents (hidden_states) and inputs:\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attention(\n\u001b[1;32m    560\u001b[0m     hidden_states,\n\u001b[1;32m    561\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    562\u001b[0m     head_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    563\u001b[0m     inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    564\u001b[0m     inputs_mask\u001b[39m=\u001b[39;49minputs_mask,\n\u001b[1;32m    565\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    566\u001b[0m )\n\u001b[1;32m    567\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/models/perceiver/modeling_perceiver.py:465\u001b[0m, in \u001b[0;36mPerceiverLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, inputs, inputs_mask, output_attentions)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    457\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    458\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    464\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 465\u001b[0m     attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    466\u001b[0m         hidden_states,\n\u001b[1;32m    467\u001b[0m         attention_mask,\n\u001b[1;32m    468\u001b[0m         head_mask,\n\u001b[1;32m    469\u001b[0m         inputs,\n\u001b[1;32m    470\u001b[0m         inputs_mask,\n\u001b[1;32m    471\u001b[0m         output_attentions,\n\u001b[1;32m    472\u001b[0m     )\n\u001b[1;32m    473\u001b[0m     attention_output \u001b[39m=\u001b[39m attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    475\u001b[0m     outputs \u001b[39m=\u001b[39m attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output attention weights\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/models/perceiver/modeling_perceiver.py:386\u001b[0m, in \u001b[0;36mPerceiverAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, inputs, inputs_mask, output_attentions)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    385\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 386\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    387\u001b[0m         hidden_states,\n\u001b[1;32m    388\u001b[0m         attention_mask,\n\u001b[1;32m    389\u001b[0m         head_mask,\n\u001b[1;32m    390\u001b[0m         inputs,\n\u001b[1;32m    391\u001b[0m         inputs_mask,\n\u001b[1;32m    392\u001b[0m         output_attentions,\n\u001b[1;32m    393\u001b[0m     )\n\u001b[1;32m    395\u001b[0m     \u001b[39m# Output projection\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m])\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/share/virtualenvs/lab-work-gFKfzxI5/lib/python3.10/site-packages/transformers/models/perceiver/modeling_perceiver.py:270\u001b[0m, in \u001b[0;36mPerceiverSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, inputs, inputs_mask, output_attentions)\u001b[0m\n\u001b[1;32m    267\u001b[0m _, _, _, v_head_dim \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[1;32m    268\u001b[0m hiddens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads \u001b[39m*\u001b[39m v_head_dim\n\u001b[0;32m--> 270\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(q_head_dim)\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     \u001b[39m# Apply the attention mask (precomputed for all layers in PerceiverModel forward() function)\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.86 GiB (GPU 0; 9.77 GiB total capacity; 6.03 GiB already allocated; 1.86 GiB free; 6.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "lab-work-gFKfzxI5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "849f61b40a9f0695613f10abe0801a9209e9c807f7f85006c3621f09dc21595f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
