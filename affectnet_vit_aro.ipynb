{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "model_ckpt = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_root = 'Affectnet/Manually_Annotated/Manually_Annotated_Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "class AffectNetDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 csvfile,\n",
    "                 root,\n",
    "                 mode='classification',\n",
    "                 crop=False,\n",
    "                 transform=None,\n",
    "                 invalid_files=None):\n",
    "        self.df = pd.read_csv(csvfile)\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.crop = crop\n",
    "        self.transform = transform\n",
    "        self.invalid_files = invalid_files\n",
    "        \n",
    "        if self.invalid_files:\n",
    "            self.df = self.df[~self.df['subDirectory_filePath'].isin(invalid_files)]\n",
    "            self.df = self.df\n",
    "        \n",
    "        self.df = self.df[~((self.df['expression'] == 9) | (self.df['expression'] == 10))].reset_index(drop=True)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = pil_loader(os.path.join(self.root, self.df['subDirectory_filePath'][idx]))\n",
    "        except KeyError:\n",
    "            raise IndexError\n",
    "        if self.crop:\n",
    "            img = img.crop((self.df['face_x'][idx],\n",
    "                            self.df['face_y'][idx],\n",
    "                            self.df['face_x'][idx]+self.df['face_width'][idx],\n",
    "                            self.df['face_y'][idx]+self.df['face_height'][idx],))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.mode == 'classification':\n",
    "            target = torch.tensor(self.df['expression'][idx])\n",
    "        elif self.mode == 'valence':\n",
    "            target = torch.tensor([self.df['valence'][idx]])\n",
    "        elif self.mode == 'arousal':\n",
    "            target = torch.tensor([self.df['arousal'][idx]])\n",
    "        else:\n",
    "            target = torch.tensor([self.df['valence'][idx],\n",
    "                                   self.df['arousal'][idx]])\n",
    "        return img.float(), target.float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('Affectnet/training.csv')\n",
    "val_df = pd.read_csv('Affectnet/validation.csv')\n",
    "\n",
    "def check_files(df):\n",
    "    invalid_files = []\n",
    "    for filename in tqdm(df['subDirectory_filePath']):\n",
    "        try:\n",
    "            pil_loader(os.path.join(images_root, filename))\n",
    "        except:\n",
    "            invalid_files.append(filename)\n",
    "    print(invalid_files)\n",
    "    return invalid_files\n",
    "\n",
    "# train_invalid_files = check_files(train_df)\n",
    "# val_invalid_files = check_files(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_invalid_files = ['103/29a31ebf1567693f4644c8ba3476ca9a72ee07fe67a5860d98707a0a.jpg']\n",
    "val_invalid_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'arousal'\n",
    "val_size = 1000\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (Compose,\n",
    "                                    Normalize,\n",
    "                                    Resize,\n",
    "                                    ToTensor)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "transform = Compose([Resize(tuple(feature_extractor.size.values())),\n",
    "                     ToTensor()])\n",
    "\n",
    "train_dataset = AffectNetDataset('Affectnet/training.csv',\n",
    "                                 images_root,\n",
    "                                 mode,\n",
    "                                 crop=False,\n",
    "                                 transform=transform,\n",
    "                                 invalid_files=train_invalid_files)\n",
    "\n",
    "val_dataset = AffectNetDataset('Affectnet/validation.csv',\n",
    "                               images_root,\n",
    "                               mode,\n",
    "                               crop=False,\n",
    "                               transform=transform,\n",
    "                               invalid_files=val_invalid_files)\n",
    "\n",
    "print('train:', len(train_dataset))\n",
    "print('validation:', len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    imgs, targets = zip(*examples)\n",
    "    pixel_values = torch.stack(imgs)\n",
    "    targets = torch.stack(targets)\n",
    "    return {'pixel_values': pixel_values, 'labels': targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4)\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                  num_labels=1,\n",
    "                                                  problem_type='regression')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced MSE\n",
    "- paper: https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Balanced_MSE_for_Imbalanced_Visual_Regression_CVPR_2022_paper.pdf\n",
    "- github: https://github.com/jiawei-ren/BalancedMSE/tree/main\n",
    "\n",
    "Batch-based Monte-Carlo (BMC)を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.distributions import MultivariateNormal as MVN\n",
    "\n",
    "def bmc_loss_md(pred, target, noise_var, device):\n",
    "    I = torch.eye(pred.shape[-1]).to(device)\n",
    "    logits = MVN(pred.unsqueeze(1), noise_var*I).log_prob(target.unsqueeze(0))\n",
    "    loss = F.cross_entropy(logits, torch.arange(pred.shape[0]).to(device))\n",
    "    loss = loss * (2 * noise_var).detach()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class BMCLoss(_Loss):\n",
    "    def __init__(self, init_noise_sigma=1., device=None, root=False):\n",
    "        super(BMCLoss, self).__init__()\n",
    "        self.noise_sigma = torch.nn.Parameter(torch.tensor(init_noise_sigma))\n",
    "        self.device = device\n",
    "        self.root = root\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        noise_var = self.noise_sigma ** 2\n",
    "        loss = bmc_loss_md(pred, target, noise_var, self.device)\n",
    "        return torch.sqrt(loss) if self.root else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class BMCLossTrainer(Trainer):\n",
    "    def __init__(self,\n",
    "                 model = None,\n",
    "                 args = None,\n",
    "                 data_collator = None,\n",
    "                 train_dataset = None,\n",
    "                 eval_dataset = None,\n",
    "                 tokenizer = None,\n",
    "                 model_init = None,\n",
    "                 compute_metrics = None,\n",
    "                 callbacks = None,\n",
    "                 optimizers = (None, None),\n",
    "                 preprocess_logits_for_metrics = None):\n",
    "        super().__init__(model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\n",
    "        self.loss_fct = BMCLoss(device=self.args.device).to(self.args.device)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import wandb\n",
    "\n",
    "wandb.init(project='AffectNet-vit', name='arousal')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"affectnet-balancedMSE-aro-no910\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=1e-3,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False,\n",
    "    report_to='wandb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, targets = eval_pred\n",
    "    rmse = mean_squared_error(targets, preds, squared=False) / 2\n",
    "    return {'rmse': rmse}\n",
    "\n",
    "# class ComputeMetrics(object):\n",
    "#     def __init__(self):\n",
    "#         self.metrics = BMCLoss(device=device).to(device)\n",
    "    \n",
    "#     def __call__(self, eval_pred):\n",
    "#         preds, targets = eval_pred\n",
    "#         preds, targets = torch.tensor(preds).to(device), torch.tensor(targets).to(device)\n",
    "#         bmse = self.metrics(preds, targets)\n",
    "#         rmse = compute_metrics(eval_pred)\n",
    "#         return {'bmse': bmse, 'rmse': rmse}\n",
    "\n",
    "# compute_bmse_metrics = ComputeMetrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = BMCLossTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # compute_metrics=compute_bmse_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()\n",
    "trainer.save_model()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def CLE_tokens(model, tokenizer, dataset, device):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for img, label in tqdm(dataset):\n",
    "        feature = tokenizer(img, return_tensors='pt').pixel_values.to(device)\n",
    "        with torch.no_grad():\n",
    "            token = model(feature, output_hidden_states=True).hidden_states[-1][0,0,:]\n",
    "        tokens.append(token.cpu())\n",
    "        labels.append(label)\n",
    "    return torch.stack(tokens).squeeze(), torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "\n",
    "def plot_tokens(tokens, targets, n_neighbors):\n",
    "    # tsne = TSNE(n_components=2)\n",
    "    # zs = tsne.fit_transform(tokens.numpy())\n",
    "    umap = UMAP(n_neighbors=n_neighbors)\n",
    "    zs = umap.fit_transform(tokens.numpy())\n",
    "    ys = targets.numpy()\n",
    "    print(zs.shape)\n",
    "    print(ys.shape)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_xlabel('feature-1')\n",
    "    ax.set_ylabel('feature-2')\n",
    "    \n",
    "    for x, y in zip(zs, ys):\n",
    "        mp = ax.scatter(x[0], x[1],\n",
    "                        alpha=1,\n",
    "                        c=y,\n",
    "                        cmap='Oranges',\n",
    "                        vmin=-1,\n",
    "                        vmax=1,\n",
    "                        s=3,)\n",
    "    fig.colorbar(mp, ax=ax)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, targets = CLE_tokens(model,\n",
    "                             feature_extractor,\n",
    "                             val_dataset,\n",
    "                             device)\n",
    "plot_tokens(tokens, targets.squeeze(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "non_finetuned_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                                num_labels=1,\n",
    "                                                                problem_type='regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, targets = CLE_tokens(non_finetuned_model.to(device),\n",
    "                             feature_extractor,\n",
    "                             val_dataset,\n",
    "                             device)\n",
    "print(tokens.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(targets), torch.max(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tokens(tokens, targets.squeeze(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-work-gFKfzxI5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
